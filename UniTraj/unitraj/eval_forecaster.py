import os
from argparse import ArgumentParser
from utils import *
import pickle
import numpy as np
from collections import defaultdict
from linear_forecaster import forecast
from lstm.lstm import LSTMModel, generate_forecasts_from_model
from av2.evaluation.forecasting.constants import CATEGORY_TO_VELOCITY_M_PER_S
from pprint import pprint 
from av2.geometry.se3 import SE3
from typing import Any, Dict, List
from uuid import UUID
from av2.datasets.sensor.sensor_dataloader import read_city_SE3_ego
from pathlib import Path
import math
from copy import deepcopy

AV2_CLASS_NAMES = ['REGULAR_VEHICLE', 'PEDESTRIAN', 'BICYCLIST', 'MOTORCYCLIST', 
                           'WHEELED_RIDER', 'BOLLARD', 'CONSTRUCTION_CONE', 'SIGN', 
                           'CONSTRUCTION_BARREL', 'STOP_SIGN', 'MOBILE_PEDESTRIAN_CROSSING_SIGN', 
                           'LARGE_VEHICLE', 'BUS', 'BOX_TRUCK', 'TRUCK', 'VEHICULAR_TRAILER', 
                           'TRUCK_CAB', 'SCHOOL_BUS', 'ARTICULATED_BUS', 'MESSAGE_BOARD_TRAILER', 
                           'BICYCLE', 'MOTORCYCLE', 'WHEELED_DEVICE', 'WHEELCHAIR', 'STROLLER', 'DOG']

Frame = Dict[str, Any]
Frames = List[Frame]
Sequences = Dict[str, Frames]
ForecastSequences = Dict[str, Dict[int, List[Frame]]]

def wrap_pi(theta) :
    theta = np.remainder(theta, 2 * np.pi)
    theta[theta > np.pi] -= 2 * np.pi
    return theta

def group_frames(frames_list: List[Dict]) -> Dict[str, List[Dict]]:
    """
    Parameters
    ----------
    frames_list: list
        list of frames, each containing a detections snapshot for a timestamp
    """
    frames_by_seq_id = defaultdict(list)
    frames_list = sorted(frames_list, key=lambda f: f["timestamp_ns"])
    for frame in frames_list:
        frames_by_seq_id[frame["seq_id"]].append(frame)
    return dict(frames_by_seq_id)

def unpack_labels(labels: List[Dict]) -> List[Dict]:
    """
    get mmdet3d (from LT3D)label file
    Returns
    -------
    list:
    """
    unpacked_labels = []
    for label in labels:
        bboxes = (
            np.array(label["gt_bboxes"])
            if len(label["gt_bboxes"]) > 0
            else np.zeros((0, 7))
        )
        velocity = (
            np.array(label["gt_velocity"])
            if len(label["gt_velocity"]) > 0
            else np.zeros((0, 2))
        )
        unpacked_labels.append(
            {
                "translation_m": bboxes[:, :3],
                "size": bboxes[:, 3:6],
                "yaw": wrap_pi(bboxes[:, 6]),
                "velocity_m_per_s": velocity,
                "label": np.array(label["gt_labels"], dtype=int),
                "name": np.array(label["gt_names"]),
                "track_id": np.array([UUID(id).int for id in label["gt_uuid"]]),
                "timestamp_ns": label["timestamp"],
                "seq_id": label["log_id"],
            }
        )
    return unpacked_labels

def read_city_SE3_ego_by_seq_id(
    dataset_dir: str, seq_ids: List[str]) -> Dict[str, Dict[int, SE3]]:
    return {
        seq_id: read_city_SE3_ego(Path(os.path.join(dataset_dir, seq_id)))
        for seq_id in seq_ids
    }
    
def transform_to_global_reference(
    detections: Dict[str, List[Dict]], city_SE3_by_seq_id: Dict[str, Dict[int, SE3]]
):
    detections = deepcopy(detections)
    for seq_id, frames in detections.items():
        for detection in frames:
            # transform xyz, velocity and yaw to city reference frame
            ego_to_city_SE3 = city_SE3_by_seq_id[seq_id][detection["timestamp_ns"]]
            detection["translation_m"] = ego_to_city_SE3.transform_from(
                detection["translation_m"]
            )  # I (number of instances), 3
            detection["ego_translation_m"] = list(
                ego_to_city_SE3.transform_from(np.zeros(3))
            )
            rotation = ego_to_city_SE3.rotation
            velocity_3d = np.pad(
                detection["velocity_m_per_s"], [(0, 0), (0, 1)]
            )  # pad last dimension -> [x, y, 0]
            detection["velocity_m_per_s"] = velocity_3d @ rotation.T  # I, 3
            ego_to_city_yaw = math.atan2(rotation[1, 0], rotation[0, 0])
            detection["yaw"] = wrap_pi(detection["yaw"] + ego_to_city_yaw)  # I

    return detections

def load(path: str) -> Any:
    """
    Returns
    -------
        object or None: returns None if the file does not exist
    """
    if not os.path.exists(path):
        return None
    with open(path, "rb") as f:
        return pickle.load(f)
    
def av2_load_predictions_and_labels(
    prediction_path: str,
    infos_path: str,
    dataset_dir: str,
):
    """
    load and convert the format of detection predictions and labels info
    prediction_path can either be the path to prediction.pkl file generated by mmdet3d or
        an detections.csv file in the AV2 detection challenge submission format
    """
    print("load labels: ", infos_path)
    raw_labels = load(infos_path)
    label_list = unpack_labels(raw_labels)
    labels = group_frames(label_list)
    city_SE3_by_seq_id = read_city_SE3_ego_by_seq_id(dataset_dir, seq_ids=labels.keys())
    labels = transform_to_global_reference(labels, city_SE3_by_seq_id)
    if prediction_path.endswith(".pkl"):
        # we are loading detection predictions generated by mmdet3d
        predictions = load(prediction_path)
        # print(sorted(predictions.keys()))
        # to have the same ordering as labels.
        # predictions = ungroup_frames(predictions)
        # predictions = group_frames(predictions) 
        
    else:
        raise Exception("prediction_path file not one of .pkl or .csv")
    return predictions, labels

if __name__ == "__main__":
    argparser = ArgumentParser()
    argparser.add_argument("--dataset", default="av2", choices=["av2", "nuscenes"])
    argparser.add_argument("--split", default="val", choices=["val", "test"])
    argparser.add_argument("--label_path", default="./dataset/av2-val/labels.pkl")
    argparser.add_argument("--pred_path", default="")
    argparser.add_argument("--datadir", default="")
    argparser.add_argument("--forecaster", default="MTR")
    argparser.add_argument("--device", default="cuda")
    
    argparser.add_argument("--top_k", type=int, default=5)
    argparser.add_argument("--ego_distance_threshold_m", type=int, default=50)
    config = argparser.parse_args()

    print("Loading prediciton: ", config.pred_path)
    with open(config.pred_path, 'rb') as f:
        forecasts = pickle.load(f)
    print("Loading labels: ", config.label_path)
    with open(config.label_path, 'rb') as f:
        labels = pickle.load(f)
    
    outputs_dir = os.path.join(
        "results", f"{config.dataset}-{config.split}", config.forecaster, "outputs")
    if config.split != "test":
        if config.dataset == "av2":
            from av2.evaluation.forecasting.eval import evaluate
            

            assert (sorted(labels.keys()) == sorted(forecasts.keys()))
            # print(sorted(set(labels.keys())))
            # print(sorted(set(forecasts.keys())))
            print("Evaluating forecasting...")
            
            res =  evaluate(forecasts, labels, config.top_k, config.ego_distance_threshold_m, config.datadir)
            mAP_F = np.nanmean([metrics["mAP_F"] for traj_metrics in res.values() for metrics in traj_metrics.values()])
            ADE = np.nanmean([metrics["ADE"] for traj_metrics in res.values() for metrics in traj_metrics.values()])
            FDE = np.nanmean([metrics["FDE"] for traj_metrics in res.values() for metrics in traj_metrics.values()])
            print("mAP_F: {}, ADE: {}, FDE: {}".format(mAP_F, ADE, FDE))
        elif config.dataset == "nuscenes":
            raise Exception(f"Not Implemented Yet")
                
        save(res, os.path.join(outputs_dir, "res.pkl"))
